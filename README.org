*Machine Learning*
  - Course:   COMP 4432 1,  COMP 3432 1, 410
  - Instructor: [[https://sites.google.com/site/poorannegi/][Pooran Singh Negi]], pooran.negi@gmail.com office 470, *Office Hours*:  M, Wed,  1.00 - 3 p.m. Email for 1-on-1 help.
  - (TA: Aiman Gannous (Aiman.Gannous@du.edu), Office 126, *office hours* Wednesday 5:30 – 6:30 PM)

*Credit:* Content on this page contain links to various external resources and images form Kevin Murhopy book  [[https://www.cs.ubc.ca/~murphyk/MLbook/][Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy]].

* Prerequisite
Student should be comfortable with linear algebra, probability, statistics,
optimization and  programming experience in python and its scientifc libraries. We will cover some basics in the class.
-  [[http://cs229.stanford.edu/section/cs229-linalg.pdf][linear algebra overview]] 
-  Read chapter 2 of Kevin Murphy for probabilty and statistics review or any other text you have used in the past
* TextBooks
- *Required:*
  -  [[https://www.cs.ubc.ca/~murphyk/MLbook/][Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy]]. Online version is available in the [[https://library.du.edu/][DU library]].
- *Optional:*  [[http://www-bcf.usc.edu/~gareth/ISL/][An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani]]. It is available online in pdf format
- *Optional:*  [[http://www.deeplearningbook.org/][Deep learning]]  by Ian Goodfellow and Yoshua Bengio and Aaron Courville.   It is available online.
- *Optional:* For more advance treatment [[https://web.stanford.edu/~hastie/ElemStatLearn/][The Elements of Statistical Learning]]. It is available online in pdf format   
* Course Description
We will go though theory behind
machine learning using tool from probability and linear algebra.
We will use python, its [[https://www.scipy.org/][scientific libraries]] (numpy, scipy, matplotlib, Pandas etc.)
and [[http://scikit-learn.org/stable/][scikit-learn: Machine Learning in Python]] during the course. For deep neural network part, we will use
highly popular [[https://www.tensorflow.org/][tensorflow Machine Intelligence]] library from the Google. For assignments, starter code  or hint will be given. 
At the end of the course one should be able to understand theory behind various
machine learning algorithms with a unifying perspective from probability theory, be comfortable using open source tools for building machine learning systems.

* Software
Please install [[https://www.anaconda.com/download/][Anaconda for Python 3.6 version data science platform. ]]Please install it before coming in the class on Tuesday.
See the youtube link [[https://www.youtube.com/watch?v=OOFONKvaz0A][Installing Anaconda, Jupyter Notebook]]. 
You can also go to my  [[https://github.com/psnegi/PythonForReproducibleResearch][python for reproducible research]]  github repository and start by running pythonBasic.ipynb notebook.
I will go over basic of python and jupyter notebook. For Deep Neural networks, we will go over tensorlfow and keras installation instruction later in the course.
** Tensorflow and Keras
 -  tensorflow related information is [[./tensorflow.org][here]]
 -  Keras related information is [[https://keras.io/][here]]
*** Try tensorflow using google colab
    - https://colab.research.google.com/notebooks/welcome.ipynb

***  deep learning resources
 - [[https://github.com/aymericdamien/TensorFlow-Examples][TensorFlow Tutorial and Examples for Beginners with Latest APIs]] by Aymeric Damien
 - [[https://github.com/fchollet/keras-resources][Keras resources]]
 - [[https://github.com/kjw0612/awesome-deep-vision][Awesome Deep Vision]]
 - [[https://github.com/kjw0612/awesome-rnn][Awesome Recurrent Neural Networks]]
   
** Python learning resources
   - [[https://try.jupyter.org/][try python notebook online without installing anything]]
   - [[http://pythontutor.com/live.html#mode%3Dedit][Runs and visualizes your python code]]
   - [[https://docs.python.org/3/tutorial/index.html][The Python Tutorial]]  
* Syllabus
*This syllabus is subject to change at the discretion of the instructor*
Here are the main topics for the class. More topics can be added as per class interest and available time.
- Basic idea of machine learning, and probability
- Generative models, parametric estimation and supervised learning.
  - Naive Bayes classifier etc.
- Gaussian models
- Linear and logistic regression
- Support vector machine, Kernels
- Matrix factorization
- Decision tree.
- Bagging, boosting and model selection etc.
- Unsupervised learning
  - Clustering etc.
- Deep learning
  - Artificial Neural Networks(ANN), End to end learning, cost function
  - Convolutional Neural Networks(CNN) for classification(image) and regression
  - Recurrent Neural Networks for natural language processing(NLP) and time series data
  - Generative adversarial networks (GANs) 

* Grading
There will be one mid term, a final exam, homework assignments, in class quizzes.
Worst 2 of your homework assignments and 2 quizzes will be dropped. Students enrolled in 
 COMP 4432 1,  will be asked to do slightly more homework questions.


|------------------------------------------------------------------+-------------|
| Homework + Quizzes                                               | 30(20+10) % |
|------------------------------------------------------------------+-------------|
| Midterm exam,  24 th July, in class                              |         20% |
|------------------------------------------------------------------+-------------|
| Final exam, 20 August, in class                                  |         25% |
|------------------------------------------------------------------+-------------|
| Final Project, 17 August 11.59 p.m                               |         15% |
|                                                                  |             |
|------------------------------------------------------------------+-------------|
| 10 minutes final presentation about project , 16 August in class |         10% |
|------------------------------------------------------------------+-------------|
|                                                                  |             |

grade range [('A', >=95), ('A_minus', >=90), ('B_plus', >=85), ('B', >=80), ('B_minus', >=75), ('C_plus', >=70), ('C', >=65), ('C_minus', >=60),
 ('D_plus', >=55), ('D', >=50), ('D_minus', >=45),  ('F', < 45)])

*Please respect DU [[https://www.du.edu/studentlife/studentconduct/honorcode.html][Honor Yourself, Honor the Code]]*

* Final Project and presentation
  Click [[./project_presentation.org][project presentation]] to see what is  expected in the presentation.
  Click [[./final_project.org][Here]] to see what is expected in final project
** Datsets for final Projects
  You can use any dataset you are interested in. Here is some listing of open datasets.
  - [[https://archive.ics.uci.edu/ml/datasets.html][UC Irvine Machine Learning Repository]]
  - [[https://www.kaggle.com/datasets][Kaggle Datasets]]  
  - [[https://github.com/niderhoff/nlp-datasets][nlp-datasets]]
  - [[https://data.worldbank.org/][World Bank Data]]
  - [[https://catalog.data.gov/dataset][U.S. Government's open data]]
  - [[https://www.census.gov/][United States Census Bureau]]
  - [[https://www.ncdc.noaa.gov/][National Climatic Data Center - NOAA]]
  - [[http://www.internationalgenome.org/data][IGSR: The International Genome Sample Resource]]


* Quiz
|------+------------|
| quiz | sol        |
|------+------------|
|    1 | [[./hws/quiz1_sol.pdf][quiz 1 sol]] |
|------+------------|
|      |            |
  
* Midterm
| Midterm | solution |
|---------+----------|
|       1 |          |
|---------+----------|

* Homework
Homework numbers are as per *Kevin Murphy ebook from the library*
| HW |            |                                                                                                                                                                                                                                      | Due date                         | sol |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|    |            |                                                                                                                                                                                                                                      |                                  |     |
|  1 | 1a         | *coding part*:  [[file:hws/hw1_python_numpy_matplotlib.ipynb][python_numpy_matplotlib]]                                                                                                                                                                                              | 29 June 11.59 p.m                | [[./hws/hw1_python_numpy_matplotlib_solutions.ipynb][sol]] |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|    | 1b         | *written part*: Problem numbers are from kevin murphy book. *Use DU  library version*.                                                                                                                                               |                                  |     |
|  1 |            | submit written solution: Chapter 2, 2.1(use bayes rule, condition on event actually observed.                                                                                                                                        | 28 June in class                 | [[./hws/HW1_sol.pdf][sol]] |
|    |            | like in part a say N_b = number of boys, N_g no of girls) (4 = 2+2 point), 2.3 (2 point), 2.4(1 point),                                                                                                                              |                                  |     |
|    |            | 2.6(6 = 3+3 point), 2.12(2 point), 2.16(3= 1+1+1 points)                                                                                                                                                                             |                                  |     |
|    |            | *Look for chapter 2 for definitions and explain various steps in the work*                                                                                                                                                           |                                  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|  2 | 2a         | Chpater 2,    2.13 (3 point, hint: I(X,Y) = H(X) + H(Y) - H(X,Y))                                                                                                                                                                    | 6 th July at 3 p.m in office 470 | [[./hws/HW2_sol.pdf][sol]] |
|    |            | chapter 3,    3.6 (4 point), 3.7(2 point each), 3.11(1 point each), 3.20(1 point each),                                                                                                                                              |                                  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|  2 | 2b         | [[./hws/implementing_naive_bayes.ipynb][implementing naive bayes ham or spam for sms]]                                                                                                                                                                                         | 9 th July online                 | [[./hws/implementing_naive_bayes_solution.ipynb][sol]] |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|    |            |                                                                                                                                                                                                                                      |                                  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|    |            |                                                                                                                                                                                                                                      |                                  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|  3 | 3 written  | Q1 (6 point)- Prove that If $\Sigma_c$ (covariance matrix for class c) is diagonal, then Gaussian discriminant analysis is equivalent to naive Bayes                                                                                 |                                  |     |
|    |            | Q2 (4 point)-                                                                                                                                                                                                                        |                                  | [[./hws/HW3_sol.pdf][sol]] |
|    |            | - From the book 4.1 (2 point )(look into section 2.5.1 for definition of correlation coefficient), 4.14(4 point 1+1+1+1 point each)                                                                                                  |                                  |     |
|    |            | 4.21(4 = 2 + 2 point ), 4.22(2 = 1+1 point),                                                                                                                                                                                         | 12 th July in class at 1.50 p.m  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|  3 | HW3_coding | [[./hws/QDA.ipynb][QDA questions notebook]]      click [[https://raw.githubusercontent.com/psnegi/ml_s2018/master/hws/QDA.ipynb][here]]  to download it                                                                                                                                                                               | 14 th July 11.59 p.m             |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|  4 | 4          | 5.2(4 = 2+2 points), 5.3(6= 3+3)(hint 5.3(b) if r/s ->0 then there in no cost of rejecting. so what should be done? For other part analyze the inequality (it become trivial even for most probable class). What should we do then?) |                                  |     |
|    |            | 5.10(2 point) I think problem should be If $L_{FN} = cL_{FP}$, show that we should pick $\hat{y}$ = 1 if $p(y =1 \text{ given } x) > \tau$ where $\tau = (c)/(1+c)$ *Let me know if you are able to show book version.*              |                                  |     |
|    |            | Question from reading section.(2 points) From the book using equation 7.30, 7.31 derive equation 7.32(ridge regression)                                                                                                              | 20 July at 4 p.m in office 470   |     |
|    |            | 7.2 (1 point)(check the formula for W in the book. X transpose is missing)                                                                                                                                                           |                                  |     |
|    |            | 7.4 (2 point)(Only if enrolled in COMP 3432)  and  7.9 (2 points)(Only if enrolled in COMP 4432)                                                                                                                                     |                                  |     |
|    |            | 8.3(5 =  1 + 2 + 2 points )                                                                                                                                                                                                          |                                  |     |
|----+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+-----|
|    |            |                                                                                                                                                                                                                                      |                                  |     |
* Course announcements
|---------+-----------------------------------|
|    Date | Announcement                      |
|---------+-----------------------------------|
| 7-14-18 | quiz 2 is scheduled on 19 th July |
|         |                                   |

* Course Lectures


| Date     | Reading assignment                                                                          | uploaded slides/notebooks                                                                                                                           |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 19 June  | Read chapter 1 of Kevin Murphy and Basic of probabilty from chapter 2 upto 2.4.1 and 2.4.6  |                                                                                                                                                     |
|          | Detail [[https://www.scipy-lectures.org/][Scipy Lecture Notes]] . Practice 1.3.1 and 1.3.2, 1.4.1 to 1.4.2.8 in Jupyter notebook | [[./lectures/lecture1_19june/ml_motivation.ipynb][what is ml? why we care ?]]                                                                                                                           |
|          |                                                                                             | [[lectures/lecture1_19june/numpy_basics.ipynb][python and numpy basics]]                                                                                                                             |
|          |                                                                                             |                                                                                                                                                     |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 21 June  | section 2.2, 2.3, 2.4[.1, .2, .3, .4, .5, .6], 2.5[.1, .2, .4], 2.6.1, 2.8 of kevin Murphy  | [[lectures/lecture2_21june/Generative_model.ipynb][notebook generative model]]                                                                                                                           |
|          | 3.1-3.2.4                                                                                   |                                                                                                                                                     |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 26 June  | Rest of chapter 3                                                                           | [[./lectures/lecture3_25june/note_probabilyt_and_information_theory.ipynb][information theory, mle and map]]                                                                                                                     |
|          |                                                                                             | Here is the [[https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-examples][link]] to mechanics of Lagrangian multiplier. For more detail see                                                                         |
|          |                                                                                             | this link at [[https://metacademy.org/graphs/concepts/lagrange_duality#focus%253Dlagrange_multipliers&mode%253Dlearn][metacademy]]. Go over free section                                                                                                       |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 28 June  | NBC Naive Bayes classifiers                                                                 |                                                                                                                                                     |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 3 July   | k. M. book 4.1 upto 4.2.5                                                                   | [[./lectures/lecture5_3july/MVN_demo.ipynb][MVN demo notebook]] ,                                                                                                                                 |
|          |                                                                                             | We covered navie bayes and looked into mutlivariate gaussian(MVG).                                                                                  |
|          |                                                                                             | Modelling class conditional densities.                                                                                                              |
|          |                                                                                             | How MVN lead to quadratic discriminant analysis(QDA) and                                                                                            |
|          |                                                                                             | linear discriminant analysis (LDA) when covariance matrices are tied i.e. $Σ_c = Σ$                                                                 |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 5th July | K. M. book 5.7 upto 5.7.2.2                                                                 | MVN in detail, started Bayesian decision theory                                                                                                     |
|          |                                                                                             |                                                                                                                                                     |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 10 July  |                                                                                             | [[./lectures/lecture10_july/bayes_decision.ipynb][bayes decision theory]]                                                                                                                               |
|          |                                                                                             | here is the [[https://raw.githubusercontent.com/psnegi/ml_s2018/master/lectures/lecture10_july/bayes_decision.ipynb][link]] to download it                                                                                                                     |
|          |                                                                                             | Covered Bayesian decision theory, confusion matrix and ROC(AUC) curve                                                                               |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 12 July  | 7.1- 7.3.3, 7.5.1                                                                           | We covered linear model and informally discussed how linear model can model non-linear model. Derived the MLE(least square solution) of parameter W |
|          | 8.1, 8.2, 8.3.1-8.3.3                                                                       | Please look into one drive link for class scribed notes.                                                                                            |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
| 17 July  | 8.3.6, 8.3.7, 8.6.3- 8.6.3.2                                                                | Covered geomtry of ridge regression($\ell_2$ regularization) and lasso($\ell_1$). First discriminative classifier (logistic regression)             |
|          |                                                                                             | [[http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf][More information about Pseudo-Inverses]]                                                                                                              |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
|          | 12.2                                                                                        | Will cover Multi-class logistic regression, and principle component analysis (PCA) for dimentionality reduction.                                    |
|          |                                                                                             |                                                                                                                                                     |
|----------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------|
|          |                                                                                             |                                                                                                                                                     |
