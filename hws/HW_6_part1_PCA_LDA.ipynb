{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook show how to do \n",
    "## 1 PCA\n",
    "- PCA on MNIST handwritten dataset.\n",
    "- Introduce some noise\n",
    "- How to project data into new basis(principle components)\n",
    "- Reconstruct data using only few principle components(Leave principle component capturing less variation in data). If noise is in later principle components direction then we have effectively denoised the images.\n",
    "\n",
    "## 2  Fisher LDA\n",
    "\n",
    "Let's start with PCA\n",
    " \n",
    "We will load mnist data set and after loading the data set intentionally put some random noise at each pixel. As we will see later we can still recognise the digit. It means signal is still dominant and noise is not among the major direction(principle component) of variation in the digits. Hence if we remove later principle compoenent we can denoise the digits.\n",
    "\n",
    "Please follow lecture note to understand operations in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train Data is of shape (60000, 28, 28). It contains 60000 digits of shape 28X28. **\n",
    "Data is loaded in **numpy  array object. It provides powerful multi dimensional array abstraction. 2d array provides matrix abstraction**. Checkout [numpy](http://www.numpy.org/) for quick numpy review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digits = load_digits()\n",
    "X = x_train\n",
    "\n",
    "Y= y_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work only 10000 data points to avoid memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train[0:10000,:]\n",
    "Y = y_train[0:10000]\n",
    "n_samples = X.shape[0]\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally if your features are measured in different units(cm. km, light year etc) then standardizing them is a good practice.\n",
    "Otherwise features measured in bigger units will dominate the variance.\n",
    "Also search when to standardise the feature for more detail explanation.\n",
    "\n",
    "**In this example we are not using StandardScaler. Each feature value is pixel intensity. Hence unit is not an issue here. I just kept it here to make sure you are aware of it. It is not the only way to standardize the data. To learn more read the sklearn documentation on standardizing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std_scale = StandardScaler().fit(X)\n",
    "#X = std_scale.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's plot some digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let set the seed so than we can reproduce the results across multiple run of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot some random digits from the dataset by creating some random indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_plot = 10\n",
    "indexes = np.random.randint(0, high=n_samples, size= [samples_to_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below how to plot images using plt.imshow. we need to reshape the data to 8x8 shape for visualization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indexes:\n",
    "    plt.imshow(X[idx,:],cmap= 'gray' )\n",
    "    plt.title('digit = {}'.format(str(Y[idx])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding some noise** Gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy = X + 60*np.random.randn(X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indexes:\n",
    "    plt.imshow(X_noisy[idx,:],cmap= 'gray' )\n",
    "    plt.title('digit = {}'.format(str(Y[idx])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can stil lreconize the digits. so noise is not the primary direction of variation\n",
    "# Let's do PCA to get rid of noise, assuming noise is not the primary direction  of variance and lies on lowest principle components direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first centralize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy= X_noisy.reshape((X.shape[0], -1))\n",
    "X_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std_scale = StandardScaler().fit(X_noisy)\n",
    "#X_noisy = std_scale.transform(X_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = X_noisy - np.mean(X_noisy, axis=0)\n",
    "print(X_c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_noisy, axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS per our discussion in the class we will use svd for doing PCA.\n",
    "\n",
    "You can read about svd in detail [here](http://www.cs.cornell.edu/courses/cs3220/2010sp/notes/svd.pdf) or read your linear algerba book for SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U, E, VT = svd(X)\n",
    "U, E, VT = np.linalg.svd(X_c, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, E.shape,VT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let check if svd does matrix factorization or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_c_recn = np.dot(U, np.diag(E))\n",
    "X_c_recn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c_recn = np.dot(X_c_recn, VT)\n",
    "X_c_recn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should get true on element wiser comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_c, X_c_recn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence column of V or row of VT are eigen vectors of $X_c^TX_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's plot the variance explained\n",
    "This will help you to decide how many components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_cumsum = np.cumsum(E)\n",
    "print(E_cumsum)\n",
    "total_variance = np.sum(E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(E_cumsum/E_cumsum[-1])\n",
    "plt.title('variance explained')\n",
    "plt.xlabel('num components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets figure out 15% percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_per = int(len(E_cumsum)*.15)\n",
    "print(index_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.plot(index_per, E_cumsum[index_per]/E_cumsum[-1], 'ro')\n",
    "plt.plot(E_cumsum/E_cumsum[-1])\n",
    "plt.title('varaince explained')\n",
    "plt.xlabel('num components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks  like using 117 component is also fine.\n",
    "\n",
    "Let's reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dim = index_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_proj= np.dot(X_noisy, VT.T[:, :reduced_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed = np.dot(X_proj,VT[:reduced_dim, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in indexes:\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize= (4,4))\n",
    "    ax1.imshow(np.reshape(X_noisy[idx,:], (28,28)),cmap= 'gray' )\n",
    "    ax1.set_title('noisy digit = {}'.format(str(Y[idx])))\n",
    "    ax2.imshow(np.reshape(X_reconstructed[idx,:], (28,28)),cmap= 'gray' )\n",
    "    ax2.set_title(' recon digit = {}'.format(str(Y[idx])))\n",
    "    # Fine-tune figure; make subplots farther from each other.\n",
    "    f.subplots_adjust(hspace=6.0, wspace = 6.0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize  first two component in project PCA space for all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_proj[:, 0], X_proj[:, 1],\n",
    "            c=Y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('principle component 1')\n",
    "plt.ylabel('principle component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "from sklearn import preprocessing\n",
    "#std_scale = preprocessing.StandardScaler().fit(X)\n",
    "#X_train_std = std_scale.transform(X)\n",
    "pca = PCA(n_components=reduced_dim, svd_solver='full')\n",
    "pca.fit(X_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca= pca.fit_transform(X_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "            c=Y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing PCA using covariance matrix(other method)\n",
    "\n",
    "One can build covariance matrix directly and calculate its eigen vectors to find principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Build covariance matrix and do eigen vector and value computation for doing PCA. Reconstruct data using only reduced_dim and plot the noisy and reconstructed images in indexes.\n",
    "\n",
    "indexes contains some random integers build at the start of the notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that eigenvalues are in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code block here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Plot  first two components in the projected PCA space using scatter plot as done earlier.\n",
    "\n",
    "It should match with svd  section plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code block here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_train\n",
    "Y= y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import  resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing 28x28 image to 8x8\n",
    "\n",
    "<font color = 'red'> following operation will take some time. Don't run this cell again and again <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_size = (8,8)\n",
    "X1 = np.zeros((X.shape[0],np.product(new_img_size) ))\n",
    "for i in range(X.shape[0]):\n",
    "    x = resize(X[i], new_img_size)\n",
    "    X1[i,:] = x.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indexes:\n",
    "    plt.imshow(np.reshape(X1[idx,:], new_img_size),cmap= 'gray' )\n",
    "    plt.title('digit = {}'.format(str(Y[idx])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(solver = 'eigen')\n",
    "lda_fit = lda.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = lda_fit.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_lda[:, 0], X_lda[:, 1],\n",
    "            c=Y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_label = np.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_class_label)\n",
    "num_classes = len(unique_class_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass LDA see section 8.6.3.2 in the book\n",
    "\n",
    "Let say we have $C$ classes\n",
    "\n",
    "- $n_k=$ number of sample in class k\n",
    "- $n = \\sum_{k=1}^C n_k$ total sample\n",
    "\n",
    "class $k$ mean $\\mu_k= \\frac{\\sum_{i:y_i= k}x_i}{n_i}$ and over all mean $\\mu= \\frac{\\sum_{i} x_i}{n}$\n",
    "\n",
    "Objective function is $$\\frac{|V^T S_B V|}{|V^TS_W V|}$$\n",
    "\n",
    "where\n",
    "\n",
    "- Within class scatter matrix  $S_w = \\sum_{k=1}^C S_k$ and $S_k=  \\sum_{i:y_i=k}((x_i - \\mu_i)(x_i -\\mu_i)^T)$\n",
    "i.e class k unscaled covariance\n",
    "\n",
    "- Between class scatter matrix $Sb =   \\sum_{k=i}^{k=C} n_k (\\mu_i - \\mu)(\\mu_i -\\mu)^T$\n",
    "\n",
    "Note maximum rank of $Sb$ is $C-1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computation of Sb and Sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Calculate Sb and Sw. Then solve for $S_b v = \\lambda S_w v$ Generalized eigen value, vector\n",
    "\n",
    "Hint \n",
    "- use from scipy import  linalg\n",
    "- **Check exactly only nine eigen values are non zero. other are almost zero as we have 10 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Eigen vectors will not be not unit length. Normalize(make unit length) them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code block here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5.  Build projection matrix using 9 eigen vectors and project data into these 9 direction. Visualize first two components as done earlier using scatter plot.\n",
    "\n",
    "Put eigen vectors along column. Make sure they are ordered by decreasing eigen value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
