{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1466096115517-bceecbfb6fde?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=427bcc1d8e2505d31a239d0de6b13f75&auto=format&fit=crop&w=1950&q=80\"  width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise will help us to get started with **navie bayes generative classifier** and using tools(pandas, sklearn etc) from python eco system.\n",
    "\n",
    "**Problem statement:** classify SMS messages as *HAM* or *SPAM* using **naive bayes** in supervised machine setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** We will use [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from UCI machine learning repository.\n",
    "\n",
    "credit:\n",
    "\n",
    "- This notebook has taken some text processing idea from\n",
    "https://radimrehurek.com/data_science_python/\n",
    "- some of the images are from https://cdn.pixabay.com\n",
    "- https://unsplash.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook with anaconda installation requires installing Textblob libray for text processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running following commands to install Textblob \n",
    "\n",
    "Here is more infomation about [textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output should be 0 after successful install\n",
    "# run this only once. Comment later\n",
    "os.system('conda install -c conda-forge textblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import requests\n",
    "import numpy as np\n",
    "import pprint # for pretty printing\n",
    "import os # listing and managing file patho\n",
    "import zipfile # for zip and unzip utilities\n",
    "import pandas # for data analysis\n",
    "import csv\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from textblob import TextBlob\n",
    "import seaborn as sb\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(data_url)\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and save the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_zip_file = 'smsspamcollection.zip'\n",
    "#http = urllib3.PoolManager()\n",
    "with open(sms_zip_file, 'wb') as out_file:\n",
    "    out_file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's verify it. \n",
    "**make sure output of following command contains smsspamcollection.zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let verify it. See how you can run linux bash command using !\n",
    "dir_listing = os.listdir('.') # list content of current directory\n",
    "print(dir_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1 (1 point): Can you complete following  code to check if sms_zip_file is present in above output. replace ? in following line with your code\n",
    "# hint get the current directory content in a list and use the *in* operator to check list membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert sms_zip_file ? , \"directory doesn't contain {}\".format(sms_zip_file) # hint look  in operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(sms_zip_file,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's list the content of the new data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMSSpamCollection file contains around 5k SMS messages. Checkout readme file for details.\n",
    "\n",
    "**Let's open this file and store line in python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('./data/SMSSpamCollection', 'r') as f:\n",
    "    sms_messages = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_messages[0:10]) # printing 10 messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is tab seperated(\\t) file with a new line in the end. Let remove new line. Note that label and actual message **ham, spam** is seperated by tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code show how to write list comprehension. We could have done this using for loop too.\n",
    "# [<some_func>(x) for x in <something> if  <some_condition_is_true>]\n",
    "sms_messages = [m.rstrip() for m in sms_messages] # we are not using if condition part\n",
    "print('Number of sms messages is {}'.format(len(sms_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check couple of messages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, msg in enumerate(sms_messages[0:20]): # see how we can slice list using : operator\n",
    "    print('message id {}  {}'.format(idx, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our  data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$ $x_i$ is sms message and $y_i$ is label(ham or spam)**. Using  this we will train(learn parameters $\\theta$ of a models(Naive bayes, Discriminant anlaysis based etc.)) and use trained model to classify new messages as ham or spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before jumping into using any machine learning model is understanding the data by **describing it's statistical attribute and visualizating samples or sample property**.\n",
    "We can use CSV file reader and try to accomplish above task. But as they say python is a language with **battery(libraries) included**. Let's use **pandas and matplotlib** libraries to do this task as cleanly as possible. What to describe and what to plot will be an essential skill we build as we do various data science or machine learning tasks. Also with time you will also built a knowledge of various packages available for different domain in python eco system. Most of the time reading blog and google search does the job of finding right libraries. Various packages for download and installation are avaiable at [PyPI - the Python Package Index](https://pypi.python.org/pypi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**\n",
    "This is [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "If you have more time look into this link [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.dEdNuDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will see how wrapping the file in pandas simplify lot of tasks\n",
    "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head(6) # there are other functions like tail and sample to check record in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to understand various attribute of the data\n",
    "\n",
    "*How many messages in each group etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How long are each messages*. See how we can attach a new column(pandas Series) to the pandas object.\n",
    "\n",
    "It uses lambda(anonymous funtion) and map tell what to do with each entries in **message** column.\n",
    "One can write a python function and pass it there too like\n",
    "\n",
    "def get_length(msg):\n",
    "\n",
    "    return len(msg)\n",
    "\n",
    "**messages['length'] = messages['message'].map(get_length)**\n",
    "\n",
    "But we will take more pythonic route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have length attribute.\n",
    "**To see the whole picture. Let plot length distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are enough messages of length upto 150 but very few messages are too long(>400).\n",
    "\n",
    "Reading  exact values and properties from graph is hard.\n",
    "Let's try to summarize this distribution(hist) of sms messages length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max** value tell us there are messages  of length 910. What are these message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Longest message is {}'.format(list(messages.message[messages.length > 900])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is there any difference in message length between spam and ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how length of ham messages is clustered rougly around [20 10] and spam messages length is clustered around 150.\n",
    "**Looks like** on average spam messages has more length.\n",
    "\n",
    "# Q2 (2 = 1+1 point): Can you write the code to summarize above per class disribution of messages length(**to be more precise about observation**). i.e. can you group messages and describe their length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "# hint use groupby and select length and use describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see  spam class has less number of example than ham class. This is called **class imbalance** issue. \n",
    "- We need to be carfeful in machine learning application about class imbalance. There are way to handle it but for current exercise let's ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3(1 point). Can you suggest some way to handle class imbalance issue. This is an open question. So don't worry about  being right or wrong. I am looking for some fresh thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*write you answer in this cell (doble click and it will be editable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer only understand scalar or vector or matrices. We need to convert text to vectors(features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "(It was a big  sought after skill before deep learning at least in vision applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our sms messages.\n",
    "\n",
    "### Bog of word model for document:\n",
    "\n",
    "In BOG  we treat document as collection of word without any order, like they are lying in bag. \n",
    "\n",
    "Two model to represent sms/document in a vector form are:\n",
    "\n",
    "- **Bernoulli document model: mes**sage is represented by a binary feature vector of absence or presence of word.\n",
    "- **Multinomial document model**: message is represented by an integer feature vector of word frequency.\n",
    "\n",
    " Later we will see\n",
    "there are better model for sentence or document representation **where words order matters**. There are model which takes into account the word order like [N-gram](https://en.wikipedia.org/wiki/N-gram) and importance of words across documents(bags) [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) etc.\n",
    "Infact Deep learning has enabled us to learn better embedding of words using context of words(co occurance).\n",
    "We will try to use them in **deep learning section** [**optional** see widely used [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)] now a days.\n",
    "\n",
    "We will use **Multinomial document model** in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to vector is bit involved and require a good understanding of NLP(natural language processing).\n",
    "\n",
    "But as we can imagine to convert a message into vector we need to\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words i.e do we care about Capital form(Cow vs cow), inflected form (\"goes\" vs. \"go\")\n",
    "3. Build a dictionary of words and map the messages into vector using this dictionary\n",
    "4. Finally train a  Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again we will use a python library [Textblob](http://textblob.readthedocs.io/en/dev/quickstart.html) to do heavy lifting for us.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that will split a message into its individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_tokens(message):\n",
    "    #message = unicode(message, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(message).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the original texts again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same messages, tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.message.head().apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With textblob, normalize words into their base form [lemmas](https://en.wikipedia.org/wiki/Lemmatisation) with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    no_punc = []\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    words =  [word.lemma for word in words]\n",
    "    for word in words:\n",
    "        line = \"\".join(char for char in word if char not in string.punctuation)\n",
    "        no_punc.append(line)\n",
    "    return no_punc    \n",
    "        \n",
    "\n",
    "# see how head portion changes\n",
    "preprocessed_messages =messages.message.apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_labels = messages.label\n",
    "messages_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how **goes** is changed to **go**. We can do some more pre processing  with some justification but let's go\n",
    "ahead with current manipulation.\n",
    "\n",
    "There are lots of other natural language processing libraries in python for performing above activities like\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [nltk](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the fundamental concern in machine learning is *Generalization*, **how well our machine is going to work well on unseen/future data. Does it generalize well on future data**?\n",
    "- If we wanted to do well on given data, why  would we even bother to build an algorithm. We can just store the data and do a lookup for any sms message.\n",
    "\n",
    "We will come back to this question later in the course when we talk about **algorithm/model selection and evaluation**.\n",
    "\n",
    "One simple way to answer above question is to hide some portion of dataset and use remaining dataset for building the model i.e. learning the parameters. Once we have build the model, we can report some number/measure on hidden dataset to tell how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's partition our data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_portion =.9 # keep 90 % data for training\n",
    "#LEt's create some random integer index and partition the data\n",
    "number_of_examples = preprocessed_messages.shape[0]\n",
    "print('Total examples are {}'.format(number_of_examples))\n",
    "np.random.seed(0) # to make sure multiple run give same result\n",
    "random_index = np.random.permutation(range(number_of_examples))\n",
    "training_set_size = int(number_of_examples*training_set_portion)\n",
    "print('train set size is {} test set size is {}'.format(training_set_size,number_of_examples - training_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_messages = preprocessed_messages[random_index[:training_set_size]]\n",
    "training_labels = messages_labels[random_index[:training_set_size]]\n",
    "test_messages = preprocessed_messages[random_index[training_set_size:]]\n",
    "test_labels = messages_labels[random_index[training_set_size:]]\n",
    "\n",
    "print('Shape of training X {} and train Y {}'.format(training_messages.shape, training_labels.shape))\n",
    "print('Shape of test X {} and test Y {}'.format(test_messages.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use training messages only for building the model**\n",
    "\n",
    "- We need to convert each message into count vector. Where a ham/spam message is mapped to vector representing each word frequency in the message\n",
    "    + To do this we need to build a dictionary of words first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4(1 point) Write code to update the python set. We are using set as it takes care of duplicate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words are in our dictionary\n",
    "unique_word = set()\n",
    "for message in training_messages:\n",
    "    # write your code here to update unique_word set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words in vocabulary(V)\n",
    "len(unique_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode each message into len(unique_word) dimensional vector. In meachine learning we call acitivity like this feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let' use default dictionary to assign each word a unique location in feature vector\n",
    "from collections import defaultdict, Counter\n",
    "word_to_index_dict = defaultdict(int)\n",
    "for index , word in enumerate(unique_word):\n",
    "    word_to_index_dict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a reverse dictionary  for mapping index to word. It will help in debugging etc.\n",
    "# See how we used dictionary comprehension\n",
    "index_to_word_dict = { value:key  for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will convert each  messages into |V| dimensional vector, where |V| is size of our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a numpy integer matrix of right size, initialized with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in training_X is our x_i\n",
    "training_X = np.zeros((len(training_messages), len(unique_word)), dtype=int)\n",
    "print(training_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 (1 point) use Counter from collections to count frequency of each word in a message. I have written rest of the code for updating frequency into right index of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go over each training message, count the words using Counter and set count in feature vector for sms \n",
    "\n",
    "for sms_no, sms in enumerate(training_messages):\n",
    "    word_freq =  # write your code here\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        index_of_word = word_to_index_dict[word]\n",
    "        training_X[sms_no][index_of_word] = freq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing code is easy :)\n",
    "## But how we check if it is correct\n",
    "## Let's do some primitive checking on a sms message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_no =3\n",
    "message_word_count = Counter(training_messages.iloc[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(training_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red' > Make sure Counter and encoding gives same results in above cell </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have successfully converted sms message into feature vector and\n",
    "# collected them in numpy matrix\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1522098543979-ffc7f79a56c4?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3deb7fa95bb0a7343a38b724cbee4b5a&auto=format&fit=crop&w=1868&q=80\" alt=\"Well done\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert ham and spam label to 1 and 0  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.tail(7)# can check from head too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our training_y value\n",
    "training_y = (training_labels.values == 'ham').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check some lable value\n",
    "training_y[-7:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our sms samples. \n",
    "\n",
    "Let review some theory and see what parameters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a sms $x_i$  to a class c= HAM or c= SPAM which has maximum value of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modelling class conditional densities we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$ i.e each example has $D$ dimentional features.\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from sms document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{ij}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values(count of words) and there are total $D$ different unique values(word). This definetly suits a **$D$ side die** situation. In our case die has $D = |V|$ sides= size of feature vector.\n",
    "\n",
    "**Infact once we have learned $P(x_{ij}|c)?$ i.e probabilites of different sides for ham and spam die,**\n",
    "** ham or spam sms generation in bag of word model is nothing but rolling ham or spam die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "<font size = 6> \n",
    "$P(x_i|c) = \\frac{n !}{\\prod_j^D x_{ij !}} P(c) \\prod^{D} P(w_j|c)^{x_{ij}} \\propto P(c) \\prod^{D} P(w_j|c)^{x_{ij}}$ \n",
    "</font>\n",
    "as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "\n",
    "- Hence the parameters are nothing  nothing but relative frequency of $w_j$ in documents of class c=SPAM or c= HAM\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "- We can sum our numpy sms_feature matrix along row or dim 0 to get total frequency of each feature for ham and spam class\n",
    "- normalize total frequency of each feature with total frequency of all the features for each class.\n",
    "- prior class  densites are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are numer of document in class k.\n",
    "\n",
    "\n",
    "\n",
    "# Let's learn the parameters for c= ham(1) and c= spam(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First estimate for ham\n",
    "\n",
    "# summing up per feature count\n",
    "training_X_ham = training_X[training_y ==1]\n",
    "print(training_X_ham.shape)\n",
    "per_feature_count =np.sum(training_X_ham, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's estimate parameters for spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 6(1 Point): Write code similary to ham class to estimate following parameter parameters_w_spam for spam class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero probability issue\n",
    "As we can see some of the probablity can be zero. It will create problem when we estimate probability of a new document in test set if that word was not in training set. \n",
    "\n",
    "If any of the term in product is zero it will result in zero product. If any of the class don't have this term then probability of this document for any class will be zero. It is an ambiguous situation. If we play log trick for comparing product of probability, we will be in troble as log of zero is not defined too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to handle this situtation to add a fake 1 count of the word in each class. This is called Laplace law of sccession or add one smoothing.\n",
    "\n",
    "We estimate\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c) + 1}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c) + |V|}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function and $|V|$ is size of our dictionary.\n",
    "\n",
    "This can be done by adding a row of ones to training_X_ham and training_X_spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New parameters Laplace law of sccession or add one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 7 (2 point) Estimate new parameter for ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "print(parameters_w_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 8(1 point). Estimate new parameter for spam class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 9 (1 point) estimate class probabilities P(c=ham) and P(c=spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = # Write your code here for estimating ham class probability\n",
    "spam = 1- ham\n",
    "ham,spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have learned the model(i.e its parameters, probabilities of different words occuring in ham dice and spam dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is our model ?\n",
    "- Let take out our test data and convert to count feature vector using same dictionary\n",
    "- Calulate the probability if test data belonging to Ham or spam. i.e if probability if >=.5 Ham otherwise spam\n",
    " or we can calulate the ratio\n",
    " <font size = 5>\n",
    " $\\frac{P(x_{test}|c=ham)}{P(x_{test}|c=spam)} = \\frac{ P(c=ham)  \\prod^{D}_{j =1} P(w_j|c=ham)^{x_{test,j}}} { P(c= spam)\\prod^{D}_{j=1} P(w_j|c=spam)^{x_{test, j}}}$ \n",
    " </font>\n",
    " \n",
    " **Note:Generally such large product of probabilties, turns out to be zero because of computer representation limits of real numbers.**\n",
    " \n",
    " Another option is let take log on right hand side and after some manipulation one can show that if\n",
    " <font size = 5>\n",
    "  $\\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=ham)) +log(P(c= ham)) \\ge log(P(c=spam))+ \\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=spam))$\n",
    "  \n",
    "  </font>\n",
    "  \n",
    " then it is ham otherwise spam\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "- Compare our prediction of label with test data label and let's report accuracy\n",
    "\n",
    "Let's do these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.zeros((len(test_messages), len(unique_word)), dtype=int)\n",
    "print(test_X.shape)\n",
    "len(word_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(sms, word_to_index_dict):\n",
    "    feature = np.zeros((len(word_to_index_dict),), dtype=int)\n",
    "    word_freq =  Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in word_to_index_dict:\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            feature[index_of_word] = freq\n",
    "    return feature        \n",
    "    \n",
    "for sms_no, sms in enumerate(test_messages):\n",
    "    test_X[sms_no] = build_feature(sms, word_to_index_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again checking feature creation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_no =2\n",
    "message_word_count = Counter(test_messages.iloc[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(test_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need test_y  value to compare predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 10 (1 point) convert ham and spam to 1 and 0 integer as done in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our test_y value\n",
    "test_y = # Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's calculate ham/spam probability for test messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_score = np.zeros_like(test_y,dtype=float)\n",
    "spam_score = np.zeros_like(test_y,dtype=float)\n",
    "ham_score.shape, spam_score.shape# just printing to make sure shape is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(parameters,test_sms, class_prior):\n",
    "    return np.sum(np.log(np.power(parameters,test_sms))) + class_prior\n",
    "\n",
    "for idx, test_sms in enumerate(test_X):# this will fetch row by row, encoded test messages\n",
    "    ham_score[idx] = calculate_score(parameters_w_ham,test_sms, np.log(ham))\n",
    "    spam_score[idx] = calculate_score(parameters_w_spam, test_sms, np.log(spam))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let print some values for visual comparision/verification\n",
    "ham_score[0:2], spam_score[0:2], test_y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the label ham(1) or spam(0)\n",
    "ham_or_spam = (ham_score >= spam_score).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_or_spam[0:5], test_y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 11(1 point). Write code to calculate accuracy. Accuracy is defined as average number of correct predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = # Write your code here\n",
    "\n",
    "print('accuracy on test set is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ham_or_spam(message):\n",
    "    feature = build_feature(message, word_to_index_dict)\n",
    "    ham_score = calculate_score(parameters_w_ham,feature, np.log(ham))\n",
    "    spam_score = calculate_score(parameters_w_spam, feature, np.log(spam))\n",
    "    \n",
    "    return 'ham' if ham_score > spam_score else 'spam'\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how it works on new spam message which is a modified  training message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ham_or_spam(' your mailbox messaging sm alert call back 09056242159 to retrieve your message'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Try some messages with same distribution as training messages to see how well it does </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Following code will show why python eco system shines.\n",
    "We will use python library sklearn to build  multinomial Naive Bayes classifier </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 12 (2 = 1 +1 point) Write code here to train MultinomialNB classifer and it accuracy in test set\n",
    "Your code should not be more than 3 lines. Check if you get same accuracy number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
