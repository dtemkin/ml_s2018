{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1466096115517-bceecbfb6fde?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=427bcc1d8e2505d31a239d0de6b13f75&auto=format&fit=crop&w=1950&q=80\"  width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise will help us to get started with navie bayes generative classifier and using tools(pandas, sklearn etc) from python eco system.\n",
    "\n",
    "**Problem statement:** classify SMS messages as *HAM* or *SPAM* using **naive bayes** in supervised machine setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** We will use [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from UCI machine learning repository.\n",
    "\n",
    "credit:\n",
    "\n",
    "- This notebook has taken text processing idea from\n",
    "https://radimrehurek.com/data_science_python/\n",
    "- some of the images are from https://cdn.pixabay.com\n",
    "- https://unsplash.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook with anaconda installation requires installing Textblob libray for text processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running following commands to install Textblob \n",
    "\n",
    "Here is more infomation about [textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output should be 0 aftger successful install\n",
    "# run this only once. Comment later\n",
    "#os.system('conda install -c conda-forge textblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import requests\n",
    "import numpy as np\n",
    "import pprint # for pretty printing\n",
    "import os # listing and managing file patho\n",
    "import zipfile # for zip and unzip utilities\n",
    "import pandas # for data analysis\n",
    "import csv\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for converting documents in word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(data_url)\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and save the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_zip_file = 'smsspamcollection.zip'\n",
    "#http = urllib3.PoolManager()\n",
    "with open(sms_zip_file, 'wb') as out_file:\n",
    "    out_file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's verify it. \n",
    "**make sure output of following command contains smsspamcollection.zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'implementing_naive_bayes.ipynb', '.ipynb_checkpoints', 'implementing_naive_bayes_solution.ipynb', 'hw1_python_numpy_matplotlib_solutions.ipynb', 'smsspamcollection.zip', 'hw1_python_numpy_matplotlib.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#Let verify it. See how you can run linux bash command using !\n",
    "dir_listing = os.listdir('.') # list content of current directory\n",
    "print(dir_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1: Can you complete following  code to check if sms_zip_file is present in above output. replace ? in following line with your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint get the current directory content in a list and use the in operator to check list membership\n",
    "#assert sms_zip_file ? , \"directory doesn't contain {}\".format(sms_zip_file) # hint look  in operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "assert sms_zip_file in os.listdir() , \"directory doesn't contain {}\".format(sms_zip_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(sms_zip_file,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's list the content of the new data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMSSpamCollection file contains around 5k SMS messages. Checkout readme file for details.\n",
    "\n",
    "**Let's open this file and store line in python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('./data/SMSSpamCollection', 'r') as f:\n",
    "    sms_messages = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'ham\\tOk lar... Joking wif u oni...\\n', \"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'ham\\tU dun say so early hor... U c already then say...\\n', \"ham\\tNah I don't think he goes to usf, he lives around here though\\n\", \"spam\\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\\n\", 'ham\\tEven my brother is not like to speak with me. They treat me like aids patent.\\n', \"ham\\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\\n\", 'spam\\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\n', 'spam\\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sms_messages[0:10]) # printing 10 messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is tab seperated(\\t) file with a new line in the end. Let remove new line. Note that label and actual message **ham, spam** is seperated by tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sms messages is 5574\n"
     ]
    }
   ],
   "source": [
    "# Following code show how to write list comprehension. We could have done this using for loop too.\n",
    "# [<some_func>(x) for x in <something> if  <some_condition_is_true>]\n",
    "sms_messages = [m.rstrip() for m in sms_messages] # we are not using if condition part\n",
    "print('Number of sms messages is {}'.format(len(sms_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check couple of messages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message id 0  ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "message id 1  ham\tOk lar... Joking wif u oni...\n",
      "message id 2  spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "message id 3  ham\tU dun say so early hor... U c already then say...\n",
      "message id 4  ham\tNah I don't think he goes to usf, he lives around here though\n",
      "message id 5  spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "message id 6  ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "message id 7  ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "message id 8  spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "message id 9  spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "message id 10  ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "message id 11  spam\tSIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "message id 12  spam\tURGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "message id 13  ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
      "message id 14  ham\tI HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "message id 15  spam\tXXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "message id 16  ham\tOh k...i'm watching here:)\n",
      "message id 17  ham\tEh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "message id 18  ham\tFine if thats the way u feel. Thats the way its gota b\n",
      "message id 19  spam\tEngland v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\n"
     ]
    }
   ],
   "source": [
    "for idx, msg in enumerate(sms_messages[0:20]): # see how we can slice list using : operator\n",
    "    print('message id {}  {}'.format(idx, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our  data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$ $x_i$ is sms message and $y_i$ is label(ham or spam)**. Using using this we will train(learn parameters $\\theta$ of a models(Naive bayes, Discriminant anlaysis based etc.)) and use trained model to classify new messages as ham or spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before jumping into using any machine learning model is understanding the data by **describing it's statistical attribute and visualizating samples or sample property**.\n",
    "We can use CSV file reader and try to accomplish above task. But as they say python is a language with **battery(libraries) included**. Let's use **pandas and matplotlib** libraries to do this task as cleanly as possible. What to describe and what to plot will be an essential skill we build as we do various data science or machine learning tasks. Also with time you will also built a knowledge of various packages available for different domain in python eco system. Most of the time reading blog and google search does the job of finding right libraries. Various packages for download and installation are avaiable at [PyPI - the Python Package Index](https://pypi.python.org/pypi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**\n",
    "This is [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "If you have more time look into this link [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.dEdNuDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see how wrapping the file in pandas simplify lot of tasks\n",
    "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head(6) # there are other functions like tail and sample to check record in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to understand various attribute of the data\n",
    "\n",
    "*How many messages in each group etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How long are each messages*. See how we can attach a new column(pandas Series) to the pandas object.\n",
    "\n",
    "It uses lambda(anonymous funtion) and map tell what to do with each entries in **message** column.\n",
    "One can write a python function and pass it there too like\n",
    "\n",
    "def get_length(msg):\n",
    "\n",
    "    return len(msg)\n",
    "\n",
    "**messages['length'] = messages['message'].map(get_length)**\n",
    "\n",
    "But we will take more pythonic route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have length attribute.\n",
    "**To see the whole picture. Let plot length distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f27f6880cf8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are enough messages of length upto 150 but very few messages are too long(>400).\n",
    "\n",
    "Reading  exact values and properties from graph is hard.\n",
    "Let's try to summarize this distribution(hist) of sms messages length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5574.000000\n",
       "mean       80.478292\n",
       "std        59.848302\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max** value tell us there are messages  of length 910. What are these message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest message is [\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\"]\n"
     ]
    }
   ],
   "source": [
    "print('Longest message is {}'.format(list(messages.message[messages.length > 900])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is there any difference in message length between spam and ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f27f69b1ac8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f27f2f8dbe0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how length of ham messages is clustered rougly around [20 10] and spam messages length is clustered around 150.\n",
    "**Looks like** on average spam messages has more length.\n",
    "\n",
    "**Q2:** Can you write the code to summarize above per class disribution of messages length(**to be more precise about observation**). i.e. can you group messages and describe their length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827.0</td>\n",
       "      <td>71.471929</td>\n",
       "      <td>58.326643</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747.0</td>\n",
       "      <td>138.676037</td>\n",
       "      <td>28.871250</td>\n",
       "      <td>13.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count        mean        std   min    25%    50%    75%    max\n",
       "label                                                                 \n",
       "ham    4827.0   71.471929  58.326643   2.0   33.0   52.0   93.0  910.0\n",
       "spam    747.0  138.676037  28.871250  13.0  133.0  149.0  157.0  223.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer\n",
    "messages.groupby('label')['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer only understand scalar or vector or matrices. We need to convert text to vectors(feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our sms message.\n",
    "\n",
    "### Bog of word model for document:\n",
    "\n",
    "In BOG  we treat document as collection of word without any order. \n",
    "\n",
    "- **Bernoulli document model: mes**sage is represented by a binary feature vector of absence or presence of word.\n",
    "- **Multinomial document model**: message is represented by an integer feature vector of word frequency.\n",
    "\n",
    " Later we will see\n",
    "there are better model for sentence or document representation **where words order matters**. There are model which takes into account the word order like [N-gram](https://en.wikipedia.org/wiki/N-gram) etc.\n",
    "Infact Deep learning has enabled us to learn better embedding of words using context of words(co occurance).\n",
    "We will try to use them in **deep learning section** [**optional** see [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to vector is bit involved and require a good understanding of NLP(natural language processing).\n",
    "\n",
    "But as we can imagine to convert a message into vector we need to\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words i.e do we care about(do they cary some infomration) Capital form(Cow vs cow), inflected form (\"goes\" vs. \"go\")\n",
    "3. Build a dictionary of words and map the messages into vector using this dictionary\n",
    "4. Finally train a  Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again we will use a python library [Textblob](http://textblob.readthedocs.io/en/dev/quickstart.html) to do heavy lifting for us.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that will split a message into its individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_tokens(message):\n",
    "    #message = unicode(message, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(message).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the original texts again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same messages, tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, until, jurong, point, crazy, Available, o...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3    [U, dun, say, so, early, hor, U, c, already, t...\n",
       "4    [Nah, I, do, n't, think, he, goes, to, usf, he...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.message.head().apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With textblob, normalize words into their base form [lemmas](https://en.wikipedia.org/wiki/Lemmatisation) with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = TextBlob(message).words\n",
    "    no_punc = []\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    words =  [word.lemma for word in words]\n",
    "    for word in words:\n",
    "        line = \"\".join(char for char in word if char not in string.punctuation)\n",
    "        no_punc.append(line)\n",
    "    return no_punc    \n",
    "        \n",
    "\n",
    "# see how head portion changes\n",
    "preprocessed_messages =messages.message.apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, do, nt, think, he, go, to, usf, he, l...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "3     ham\n",
       "4     ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_labels = messages.label\n",
    "messages_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how **goes** to changed to **go**. We can do some more pre processing  with some justifucation but let's go\n",
    "ahead with current manipulation.\n",
    "\n",
    "There are lots of other natural language processing libraries in python for performing above activities like\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [nltk](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the fundamental concenrm in machine learning is *Generalization*, **how well our machine is going to work well on unseen/future data. Does it generalize well on future data**?\n",
    "- If we wanted to do well on given data, why  would we even bother to build an algorithm. We can just store the data and do a lookup for any sms message.\n",
    "\n",
    "We will come back to this question later in the course when we talk about **alogorith/model selection and evaluation**.\n",
    "\n",
    "One simple way to unswer above question is to hide some portion of dataset and use remaining dataset for building the model i.e. learning the parameters. Once we have build the model, we can report some number/measure on hidden dataset to tell how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's partition our data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples are 5574\n",
      "train set size is 5016 test set size is 558\n"
     ]
    }
   ],
   "source": [
    "training_set_portion =.9 # keep 80 % data for traning\n",
    "#LEt's create some random integer index and partition the data\n",
    "number_of_examples = preprocessed_messages.shape[0]\n",
    "print('Total examples are {}'.format(number_of_examples))\n",
    "np.random.seed(0) # to make sure multiple run give same result\n",
    "random_index = np.random.permutation(range(number_of_examples))\n",
    "training_set_size = int(number_of_examples*training_set_portion)\n",
    "print('train set size is {} test set size is {}'.format(training_set_size,number_of_examples - training_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X (5016,) and train Y (5016,)\n",
      "Shape of test X (558,) and test Y (558,)\n"
     ]
    }
   ],
   "source": [
    "training_messages = preprocessed_messages[random_index[:training_set_size]]\n",
    "training_labels = messages_labels[random_index[:training_set_size]]\n",
    "test_messages = preprocessed_messages[random_index[training_set_size:]]\n",
    "test_labels = messages_labels[random_index[training_set_size:]]\n",
    "\n",
    "print('Shape of training X {} and train Y {}'.format(training_messages.shape, training_labels.shape))\n",
    "print('Shape of test X {} and test Y {}'.format(test_messages.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use training messages only for building the model**\n",
    "\n",
    "- We need to convert each message into count vector. Where a ham/spam message is mapped to vector representing each word frequency in the message\n",
    "    + To do this we need to build a dictionary of words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words are in our dictionary\n",
    "unique_word = set()\n",
    "for message in training_messages:\n",
    "    unique_word.update(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8316"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in vocabulary\n",
    "len(unique_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode each message into **7798** dimentional vector. In meachine earning we call acitivity like this feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let' use default dictionary to assign each word a unique location in feature vector\n",
    "from collections import defaultdict, Counter\n",
    "word_to_index_dict = defaultdict(int)\n",
    "for index , word in enumerate(unique_word):\n",
    "    word_to_index_dict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a reverse dictionary  for mapping index to word. It will help in debugging etc.\n",
    "# See how we used dictionary comprehension\n",
    "index_to_word_dict = { value:key  for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n"
     ]
    }
   ],
   "source": [
    "print(training_messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will convert each 4459 messages into 7798 dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a numpy integer matrix of 4459X7798, initialized with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016, 8316)\n"
     ]
    }
   ],
   "source": [
    "# each row in training_X is our x_i\n",
    "training_X = np.zeros((len(training_messages), len(unique_word)), dtype=int)\n",
    "print(training_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go over each training message, count the words using Counter and set count in feature vector for sms \n",
    "\n",
    "for sms_no, sms in enumerate(training_messages):\n",
    "    word_freq =  Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        index_of_word = word_to_index_dict[word]\n",
    "        training_X[sms_no][index_of_word] = freq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing code is easy :)\n",
    "## But how we check if it is correct\n",
    "## Let's do some primitive checking on a sms message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'if': 2, 'you': 2, 'ok': 1, 'can': 1, 'be': 1, 'later': 1, 'showing': 1, 'around': 1, '8830': 1, 'want': 1, 'cld': 1, 'have': 1, 'drink': 1, 'before': 1, 'wld': 1, 'prefer': 1, 'not': 1, 'to': 1, 'spend': 1, 'money': 1, 'on': 1, 'nosh': 1, 'do': 1, 'nt': 1, 'mind': 1, 'a': 1, 'doing': 1, 'that': 1, 'nxt': 1, 'wk': 1})\n",
      "##Encoding for sms no 3 in feature vector is ##\n",
      "ok 1\n",
      "that 1\n",
      "do 1\n",
      "prefer 1\n",
      "nosh 1\n",
      "nt 1\n",
      "wld 1\n",
      "around 1\n",
      "wk 1\n",
      "have 1\n",
      "drink 1\n",
      "nxt 1\n",
      "be 1\n",
      "before 1\n",
      "showing 1\n",
      "spend 1\n",
      "on 1\n",
      "a 1\n",
      "later 1\n",
      "mind 1\n",
      "want 1\n",
      "can 1\n",
      "8830 1\n",
      "cld 1\n",
      "if 2\n",
      "you 2\n",
      "money 1\n",
      "not 1\n",
      "to 1\n",
      "doing 1\n"
     ]
    }
   ],
   "source": [
    "sms_no =3\n",
    "message_word_count = Counter(training_messages.iloc[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(training_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red' > Make sure Counter and encoding gives same results in above cell </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have successfully converted sms message into feature vector and\n",
    "# collected them in numpy matrix\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1522098543979-ffc7f79a56c4?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3deb7fa95bb0a7343a38b724cbee4b5a&auto=format&fit=crop&w=1868&q=80\" alt=\"Well done\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert ham and spam label to 1 and 0  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201     ham\n",
       "4260     ham\n",
       "424     spam\n",
       "4421     ham\n",
       "3715     ham\n",
       "664      ham\n",
       "350      ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.tail(7)# can check from head too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our training_y value\n",
    "training_y = (training_labels.values == 'ham').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check some lable value\n",
    "training_y[-7:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our sms samples. \n",
    "\n",
    "Let review some theory and see what parameters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a sms $x_i$  to a class c= HAM or c= SPAM which has maximum vlaue of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modelling class conditional densities we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from sms document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{ij}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values and there are total $D$ different unique values(word). This definetly suits a **$D$ side die** situation. In our case die has $D = 8771$ sides= size of feature vector.\n",
    "\n",
    "**Infact once we have learned $P(x_{ij}|c)?$ i.e probabilites of different sides for ham and spam die,**\n",
    "** ham or spam sms generation in bag of word model is nothing but rolling ham or spam die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "<font size = 6> \n",
    "$P(x_i|c) = \\frac{n_i}{\\prod_j^D x_{ij|C}} P(c) \\prod^{D} P(w_j|c)^{x_{ij}} \\propto P(c) \\prod^{D} P(w_j|c)^{x_{ij}}$ \n",
    "</font>\n",
    "as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "\n",
    "- Hence the parameters are nothing  nothing but relative frequency of $w_j$ in documents of class c=SPAM or c= HAM\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "- We can sum our numpy sms_feature matrix along row or dim 0 to get total frequency of each feature for ham and spam class\n",
    "- normalize total frequency of each feature with total frequency of all the features for each class.\n",
    "- prior class  densites are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are numer of document in class k.\n",
    "\n",
    "- So we have to learn 8771 parameters for each die\n",
    "- and 2  class densities.\n",
    "\n",
    "\n",
    "# Let's learn the parameters for c= ham(1) and c= spam(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4346, 8316)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 7.86114081e-05, 1.57222816e-05, ...,\n",
       "       1.57222816e-05, 2.51556506e-04, 1.57222816e-05])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First estimate for ham\n",
    "\n",
    "# summing up per feature count\n",
    "training_X_ham = training_X[training_y ==1]\n",
    "print(training_X_ham.shape)\n",
    "per_feature_count =np.sum(training_X_ham, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's estimate paramters for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 8316)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.20155039e-05, 4.34108527e-04, 0.00000000e+00, ...,\n",
       "       0.00000000e+00, 6.82170543e-04, 0.00000000e+00])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summing up per feature count\n",
    "training_X_spam = training_X[training_y ==0]\n",
    "print(training_X_spam.shape)\n",
    "per_feature_count =np.sum(training_X_spam, axis = 0)\n",
    "per_feature_count.shape\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_spam = per_feature_count/(np.sum(per_feature_count))\n",
    "parameters_w_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero probability issue\n",
    "As we can see some of the probablity can be zero. It will create problem when we estimate probability of a new document in test set if that wor was not in training set. \n",
    "\n",
    "If any of the term in product is zero it will result in zero product. If any of the class don't have this term then probability of this document for any class will be zero. If we play log trcik for comparing product of probability, we will be in troble as log of zero not defined too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to handle this situtation to add a fake 1 count of the word in each class. This is called Laplace law of scession or add one smoothing.\n",
    "\n",
    "We estimate\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c) + 1}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c) + |V|}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function and $|V|$ is size of our dictionary.\n",
    "\n",
    "This can be done by adding a row of ones to training_X_ham and training_X_spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.39043382e-05, 8.34260289e-05, 2.78086763e-05, ...,\n",
       "       2.78086763e-05, 2.36373749e-04, 2.78086763e-05])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X_ham1 = np.concatenate((training_X_ham, np.ones((1,training_X_ham.shape[1]), dtype= int)), axis =0)\n",
    "\n",
    "per_feature_count =np.sum(training_X_ham1, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.18297124e-05, 3.27318849e-04, 4.09148562e-05, ...,\n",
       "       4.09148562e-05, 4.90978274e-04, 4.09148562e-05])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X_spam1 = np.concatenate((training_X_spam, np.ones((1,training_X_spam.shape[1]), dtype= int)), axis =0)\n",
    "\n",
    "per_feature_count =np.sum(training_X_spam1, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_spam = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8664274322169059, 0.13357256778309412)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham = len(training_X_ham)/(len(training_X_ham) + len(training_X_spam))\n",
    "spam = 1- ham\n",
    "ham,spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have learned the model(i.e its parameters, probabilities of different words occuring in ham die and spam die)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is our model ?\n",
    "- Let take out our test data convert to count feature vector using same dictionars\n",
    "- Calulate the probability if test data belonging to Ham or spam. i.e if probability if >=.5 Ham otherwise spam\n",
    " or we can calulate the ratio\n",
    " <font size = 5>\n",
    " $\\frac{P(x_{test}|c=ham)}{P(x_{test}|c=spam)} = \\frac{ P(c=ham)  \\prod^{D}_{j =1} P(w_j|c=ham)^{x_{test,j}}} { P(c= spam)\\prod^{D}_{j=1} P(w_j|c=spam)^{x_{test, j}}}$ \n",
    " </font>\n",
    " \n",
    " **Note:Generally such large product of probabilties, turns out to be zero because of computer representation limits of real numbers.**\n",
    " \n",
    " Another option is let take log on right hand side and after some manipulation one can show that if\n",
    " <font size = 5>\n",
    "  $\\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=ham)) +log(P(c= ham)) \\ge log(P(c=spam))+ \\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=spam))$\n",
    "  \n",
    "  </font>\n",
    "  \n",
    " then it is ham other wise spam\n",
    " \n",
    " \n",
    " \n",
    " if it is bigger than one then it is ham , otherwise spam\n",
    " \n",
    "- Compare with test data label and let's report accuracy and confusion matrix\n",
    "\n",
    "Let's do these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 8316)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8316"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.zeros((len(test_messages), len(unique_word)), dtype=int)\n",
    "print(test_X.shape)\n",
    "len(word_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8316,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = np.zeros((1, len(word_to_index_dict)), dtype=int)\n",
    "feature.shape\n",
    "test_X[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(sms, word_to_index_dict):\n",
    "    feature = np.zeros((len(word_to_index_dict),), dtype=int)\n",
    "    word_freq =  Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in word_to_index_dict:\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            feature[index_of_word] = freq\n",
    "    return feature        \n",
    "    \n",
    "for sms_no, sms in enumerate(test_messages):\n",
    "    test_X[sms_no] = build_feature(sms, word_to_index_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again checking feature creation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'hello': 1, 'they': 1, 'are': 1, 'going': 1, 'to': 1, 'the': 1, 'village': 1, 'pub': 1, 'at': 1, '8': 1, 'so': 1, 'either': 1, 'come': 1, 'here': 1, 'or': 1, 'there': 1, 'accordingly': 1, 'ok': 1})\n",
      "##Encoding for sms no 2 in feature vector is ##\n",
      "either 1\n",
      "going 1\n",
      "there 1\n",
      "are 1\n",
      "ok 1\n",
      "the 1\n",
      "here 1\n",
      "village 1\n",
      "8 1\n",
      "hello 1\n",
      "they 1\n",
      "come 1\n",
      "or 1\n",
      "accordingly 1\n",
      "at 1\n",
      "pub 1\n",
      "so 1\n",
      "to 1\n"
     ]
    }
   ],
   "source": [
    "sms_no =2\n",
    "message_word_count = Counter(test_messages.iloc[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(test_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need test_y this value to compare labels and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our test_y value\n",
    "test_y = (test_labels.values == 'ham').astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's calculate ham/spam probability for test messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((558,), (558,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_score = np.zeros_like(test_y,dtype=float)\n",
    "spam_score = np.zeros_like(test_y,dtype=float)\n",
    "ham_score.shape, spam_score.shape# just printing to make sure shape is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(parameters,test_sms, class_prior):\n",
    "    return np.sum(np.log(np.power(parameters,test_sms))) + class_prior\n",
    "\n",
    "for idx, test_sms in enumerate(test_X):# this will fetch row by row, encoded test messages\n",
    "    ham_score[idx] = calculate_score(parameters_w_ham,test_sms, np.log(ham))\n",
    "    spam_score[idx] = calculate_score(parameters_w_spam, test_sms, np.log(spam))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -67.64849451, -155.29533957]),\n",
       " array([ -85.58871614, -135.47357743]),\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_score[0:2], spam_score[0:2], test_y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_or_spam = (ham_score >= spam_score).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 0, 1]), array([1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_or_spam[0:5], test_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -67.64849451, -155.29533957, -121.14388609]),\n",
       " array([ -85.58871614, -135.47357743, -137.02230242, -196.53240289,\n",
       "        -155.6703781 ]),\n",
       " -0.1433769214963817,\n",
       " -2.0131103699277446)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if calculation was right\n",
    "ham_score[0:3], spam_score[0:5], np.log(ham), np.log(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.982078853046595\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean((ham_or_spam == test_y))\n",
    "\n",
    "print('accuracy on test set is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ham_or_spam(message):\n",
    "    feature = build_feature(message, word_to_index_dict)\n",
    "    ham_score = calculate_score(parameters_w_ham,feature, np.log(ham))\n",
    "    spam_score = calculate_score(parameters_w_spam, feature, np.log(spam))\n",
    "    \n",
    "    return 'ham' if ham_score > spam_score else 'spam'\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how it works on new spam message which is a modified  training message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ham_or_spam(' your mailbox messaging sm alert call back 09056242159 to retrieve your message'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Try some messages with same distribution as training messages to see how well it does </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Following code will show why python eco system shines.\n",
    "We will use python library sklearn to build  multinomial Naive Bayes classifier </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use CountVectorizer from **sklearn** to convert each  message into **count vector**.   Any row of this matrix represents an example(count of various words in the message).\n",
    "\n",
    "**Let's create the transformation class first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(training_X, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982078853046595"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_X, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
