{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we will explore Quadratic Discriminant Analysis (QDA) in 2D using iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris virginica\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/Southern_Blue_Flag_Iris_%28iris_virginica%29_-_Flickr_-_Andrea_Westmoreland.jpg\" alt=\"Smiley face\" height=\"400\"  width=\"400\">\n",
    "## Iris versicolor\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\" alt=\"Smiley face\" height=\"400\"  width=\"400\">\n",
    "## Iris setosa\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/11/Iris_setosa_2.jpg\" alt=\"Smiley face\" height=\"400\"  width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's download dataset $\\mathcal{D} = \\{(\\mathbf{x_i}, y_i) \\}_{i=1}^{N}$ containing features $\\mathbf{x_i}$ of these flowers $y_i$ from UCI machine learning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for doing eploratory data analysis\n",
    "import seaborn as sns # statistical visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "# to make graphics inline\n",
    "%matplotlib inline \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using pandas read_csv and giving name for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "iris_df = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's randomly sample 5 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)# randomly looking at five samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Summary and data sanity check\n",
    "\n",
    "Please read pandas **isnull** and **any** functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make sure values in different columns are not missing\n",
    "iris_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As per the above output none of the columns have  any null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making sure datatype is also good, so that relevant algebra on columns make sense\n",
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class type is object(string). Down the line, we need to convert it to integer label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In any machine learning task we want to do good on future data so let's split our data into train and test using sklearn. Last time we wrote splitting logic but all these activities are so routine that sklearn has inbult functions for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1 (1 point), Partition the data into train and test test using train_test_split function from sklearn. Use following  seed parameter too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.32\n",
    "seed = 3\n",
    "# write your code  to complete following line\n",
    "train_df, test_df = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training observation {}'.format(train_df.shape))\n",
    "print('total teasting observation {}'.format(test_df.shape))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence ignoring the class label, we have 4 features for each type of flower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how many flowers per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we have almost equal examples in each class. Class imbalance is not a big issue but let's still take care of it(We need to estimate class prior probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per above summary most the real values featues are clustered around their mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize features to find 2 most discriminative one as we want to model class conditional densities using 2D gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use matplotlib to do scatter plot(kind of Multivariate Plot) to see which features are most discriminative one but seaborn draws attractive statistical graph and we can focus more time on our main objective(find 2 most discriminative one )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.pairplot(train_df, hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pairplot,  plots scatter plot off diagonal and histogram(1 d distribution) of various attributes in diagonal.\n",
    "## read the corresponding row and columns\n",
    "\n",
    "Scatter plot gives a vague idea of the 2D distribution of the corresponding features. Points gets plotted on top of each other and distribution of the points may not be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like any two pair of raw features can't classify flowers perfectly\n",
    "\n",
    "As we want to explore 2D QDA, let's choose **petal length and petal width(see 3,4 entry in above pair plot)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following figures gives much better idea of density although it is fitted density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name, per_class_df in train_df.groupby('class'):\n",
    "    print('{} \\n'.format(class_name))\n",
    "    sns.jointplot(x= 'petal_length', y = 'petal_width',kind='kde' , data = per_class_df)\n",
    "    plt.show()\n",
    "#train_df.hist(by= 'class', figsize = (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we want to work with only 2 features(petal_lenght, petal_width), we need to select only these column from training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the operations  in pandas are performed on underlying  numpy array. We can get this array by using values property\n",
    "# Most of the operations(mean, variance, covariance matrix) can be perfomed using pandas but let's use numpy\n",
    "X_train = train_df[['petal_length', 'petal_width']].values\n",
    "y_train = train_df['class']\n",
    "X_test = test_df[['petal_length', 'petal_width']].values\n",
    "y_test = test_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some debugging information\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[['petal_length', 'petal_width']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how 5 the training record looks now\n",
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversion to numpy looks good\n",
    "\n",
    "<font color = 'red'> Make sure pandas train_df and X_train agree on values  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience let's convert these string labels to integers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before mapping\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping class labels to integer in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary of mapping\n",
    "mapping = {v:k for k, v in enumerate(y_train.unique())}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after mapping\n",
    "y_train = y_train.map(mapping)\n",
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do same of test label\n",
    "y_test = y_test.map(mapping).values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA steps\n",
    "- Let's fit(learn mean and covariance matrix) 2D gaussian to joint distribution of (petal_length, petal_width) for different flower category\n",
    "- Once we have learned per class mean and covariance matrix, we can build discriminant function for discriminating(classifying) future iris flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From MLE estimate per class mean is empirical mean and covariance matrix is empirical covariance matrix\n",
    "<font size = 7>\n",
    "$\\mathbf{\\mu_c} = \\sum_{i=1; y_i = c}^{N} \\frac{\\mathbf{x_i}}{\\#I(y_i ==c)}$ \n",
    "\n",
    "$\\Sigma_c = \\sum_{i=1; y_i =c}^{N} \\frac{\\mathbf{(x_i -\\mu_c) (x_i -\\mu_c)^{T}}}{\\#I(y_i ==c)}$\n",
    "\n",
    "<br>\n",
    "$\\pi_c = \\frac{N_c}{N}$ where $N_c$ is number of example in class $c$ and $N$ total number of example.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2(3= 1+1+1 point) calculate the mean, inverse of covariance matrix and class prior\n",
    "we are storing inverse of covariance matrix as we need inverse matrix in discriminant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_mean_vector = []\n",
    "per_class_covariance_matrix = []\n",
    "per_class_prior = []\n",
    "for cls_idx in mapping.values():\n",
    "    #calculate cls_idx mean and covariance for each class data\n",
    "    X_c = X_train[y_train == cls_idx]\n",
    "    cls_prior = ## ??? write your code here\n",
    "    cls_mean = ## ??? write your code here\n",
    "    cls_cov = ## ??? write code here\n",
    "    per_class_mean_vector.append(cls_mean)\n",
    "    # Let's store inv as we will see later\n",
    "    # to take a inverse we need to use linalg from numpy\n",
    "    per_class_covariance_matrix.append(np.linalg.inv(cls_cov))\n",
    "    per_class_prior.append(cls_prior)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some debugging check before we jump into coding discriminant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =5 color=\"red\"> Make sure per_class_prior output and relative frequency matches. In general it is good habit to keep checking your calculation/code. It can save you a lot of time and trouble. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want to classify(discriminate) example in test set i.e we want to evaluate probablity $P(y=c|x_{test})$, This is our discriminant function.\n",
    "<font size = 5>\n",
    "$P(y=c|x_{test}) = \\frac{P(x_{test}|y=c) P(y=c)}{P(x_{test})}$\n",
    "</font>\n",
    "\n",
    "- As we have talked in the lecture, for each class discriminant function scaling by same value, adding a constant or taking log doesn't matter.\n",
    "- On right hand side, we have modelled $P(x_{test}|y=c)$ with 2 -d gaussian density and have already estimated its paramters(mean, covariane) for each class using MLE\n",
    "\n",
    "# After some algebraic simplification,  D=2 quadratic discriminant function for $\\mathbf{x} \\in \\mathbb{R}^2$ will look like\n",
    "<font size = 5>\n",
    "\n",
    "$g(\\mathbf{x}) = -(x- \\mu_c)^T \\Sigma_C^{-1}( x- \\mu_c) + \\log (det(\\Sigma_c^{-1})) + \\log \\pi_c $\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 (4 point) code disriminant function  which takes class mean , covariance matrix, class prior and a test example. Output the value of distriminant function\n",
    " - 2 = 1+1 point for using numpy dot function twice\n",
    " - 1 point for making sure you took the log\n",
    " - 1 point to make sure you took the determinant using linalg from numpy\n",
    " \n",
    "We will use this discriminant function for predicting class label on test sample. We will compare predicted label with test sample label to see how well we did?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant(mean, cov_inv,prior, x):\n",
    "    '''\n",
    "    args::\n",
    "        mean: 1-d numpy mean  vector\n",
    "        cov_inv: 2-d numpy covariance matrix inverse\n",
    "        prior: class prior probability\n",
    "        x: feature vector\n",
    "    return:\n",
    "            scalar discriminant score of x\n",
    "    ''' \n",
    "    score =  ## ??? are per above formula write the code here\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each example let's calculate this score and store in numpy array\n",
    "\n",
    "score_mat = np.zeros((len(X_test), len(mapping)))\n",
    "score_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, test_example in enumerate(X_test):\n",
    "    for cls, (mean, cov_inv, prior) in enumerate(zip(per_class_mean_vector, per_class_covariance_matrix, per_class_prior)):\n",
    "        score_mat[idx][cls] = discriminant(mean, cov_inv, prior, test_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please read about python *enumerate, zip* inbuilt function  if you don't know how they work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# based on these discriminant values lets try to predict class labels\n",
    "see how **numpy argmax** function gives indice of the largest value. The way we build mapping and stored the score, indices 0,1,2 encode class label for different flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = np.argmax(score_mat, axis=1)\n",
    "# hence accuracy is\n",
    "np.mean(predicted_label == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 (2 point) Fit the QDA using QuadraticDiscriminantAnalysis and report accuracy on test set\n",
    "<font color = 'red'>your code should not be more than three line. Hopefully your answer matches with previous value </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? write your  code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side note: One check various parameters learned by QuadraticDiscriminantAnalysis. Let check mean and our mean vector. They must match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.means_ # if do clf. and press tab key you can see all the attribute and functions this class has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this link http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
