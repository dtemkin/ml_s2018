{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook you will learn how to use build SVM for binary classification and how to use quadratic optimization package CVXOPT in python.\n",
    "\n",
    "Side Note. We don't have time to go into details about following things but I want you to be aware of following important usage of svm in practice.\n",
    "- SVM supports regression too. Look into literature for regression usage\n",
    "- SVM supports multiclass classification too\n",
    "- We can combine muliple kernel to use data from different sources(video, image ,audio) and using right kernel to measure similarity and then combine.  See MKL https://en.wikipedia.org/wiki/Multiple_kernel_learning \n",
    "  \n",
    "- **One class SVM** are used for novelty (anamolies, outlier, noise etc) detection in fraud detection, medical abnormality, production abnormality etc. One can also use **ensemble.IsolationForest** from ensemble methods.\n",
    "- See this paper for a review of novelty detection methods. http://www.robots.ox.ac.uk/~davidc/pubs/NDreview2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's review some theory. Following formulation of SVM uses Dual formualtion of the problem. See following link to see the derivation. You need to understand  optimization theory too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement  SVM. We know that SMV primal objective is\n",
    "\n",
    "$\\min_{w, w_0} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{N} \\xi_i$ st $y_i(w^T +w_0) \\gt 1 $  for $\\forall i$\n",
    "\n",
    "Note the label is $(+1, -1)$ instead of $(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Infact Dual problem(using Lagrangian) is written as \n",
    "$\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j(x_i^T x_j) - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\min_{\\alpha} \\frac{1}{2}\\alpha^T K \\alpha - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "where  $\\frac{1}{2}\\alpha^T K \\alpha$ = $\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j(x_i^T x_j)$ and $\\alpha$ is vector of $\\alpha_i$. Similar interpretation for $y$ and $y_i$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you review optimization theory, you can look at the derivation here\n",
    "http://cs229.stanford.edu/notes/cs229-notes3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "dual problem depends only on innner product $x_i^Tx_j$ between $x_i, x_j$ . Hence we can replace it with a mercer kernal $\\mathcal{k}(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$ where $\\phi$ is the function\n",
    "guranteed by mercer theorem. You can think of it as a feature mapping function $x_i \\rightarrow \\phi(x_i)$.\n",
    "For kernel svm replace matrix K  using RBF kernel i.e $K_{ij} = y_i  y_j \\mathcal{k}(x_i,x_j)$ where  $\\mathcal{k}$ is any kernel. **Use RBF kernel for this assignment.**\n",
    "\n",
    "Complete dual problem in dual form is\n",
    "$\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j \\mathcal{k}(x_i, x_j) - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cvxopt Quadratic program solver (QP).\n",
    "\n",
    "cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])\n",
    "\n",
    "which solves the problem\n",
    "\n",
    "\n",
    "$\\min_{x} \\frac{1}{2}x^T P x {\\bf+} q^T x$ s.t $Gx \\preceq h$ and $Ax = b$. Note $\\preceq$ denotes componentwise inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we need to map to this interface. Clearly $x$ is $\\alpha$ and $K$ is $P$, $q$ is vector of -1.\n",
    "\n",
    "Now we have to build  inequality\n",
    "\n",
    "**(i)** $\\alpha_i \\ge 0$ or $ -\\alpha_i < 0 $ for all $i$ or in matrix \n",
    "form $- I\\alpha \\preceq {\\bf 0}$ where $I$ is  identity matrix of size $N \\times N$ and  ${\\bf 0}$ is vector of zeros of size N(number of samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "Hence $G $ will be $\\begin{bmatrix} -I  \\end{bmatrix}$  and $h$ is $\\begin{bmatrix} {\\bf 0}   \\end{bmatrix} $\n",
    "\n",
    "\n",
    "$A$  is $y^T$ and $b$ is $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you solve for $\\alpha$, w is equal to $\\sum_{i} \\alpha_i y_i \\phi(x_i)$ and $w_0$  is equal to $\\frac{max_{j:y_j =-1} \\sum_{i} \\alpha_i y_i k(x_i, x_j) +  min_{j:y_j =1} \\sum_{i} \\alpha_i y_i k(x_i, x_j)}{2}$ \n",
    "\n",
    "use the sign$(w^T x_{test} + w_0)$  or  sign$(\\sum_{i}^{N} \\alpha_i  y_i \\mathcal{k}(x_i,x_{test}) + w_0)$ to predict label of test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's install  cvxopt package for optimization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'> Please run following cell only once.  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('conda install cvxopt')\n",
    "# You can run the command inside string from command prompt too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Click here to learn more about [Python Software for Convex Optimization](http://cvxopt.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y= data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert dataset label to {-1, +1}\n",
    "We need to convert 0 to -1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1= np.where(y==0, -1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y1 ==1), sum(y1 ==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are ignoring data imbalance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X1 = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_val_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will Implement Kernel SVM using solver from cvxopt. Use validation set to select best model and  evaluate the model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  rbf_kernel (x1,x2, sigma):\n",
    "    return np.exp(- np.linalg.norm(x1- x2)**2/(2.0* sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let declare matrix K\n",
    "# We need to build it different sigma\n",
    "K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "print(X_train.shape, K.shape)\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q= cvxopt.matrix(-1*np.ones(n_samples))\n",
    "q.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 (1 point) build matrix  G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -I \n",
    "G = ## Write code here\n",
    "G.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (1 point) build matrix h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = ## wrire code here\n",
    "h.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note y_train contains integer label value (1, -1) but in cvopt library A needs to be double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = cvxopt.matrix(np.expand_dims(y_train, axis=0), tc = 'd')\n",
    "A.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= cvxopt.matrix(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(x, svec_alphas, svecs, svec_labels, wo, sigma):\n",
    "    val = wo\n",
    "    for alpha, svec, lbl in  zip(svec_alphas, svecs, svec_labels):\n",
    "        val +=  alpha*lbl*rbf_kernel(x,svec, sigma )\n",
    "        \n",
    "    return np.sign(val)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In RBF kernel $k(x_1, x_1) = \\exp(\\frac{\\|x_1 -x_2\\|^2}{-2 \\sigma^2})$. Also written as  $ \\exp(- \\gamma \\|x_1 -x_2\\|^2)$ there is a hyper parameter $\\sigma$ which we need to decide before solving the  problem using optimizer.\n",
    "\n",
    "- For low value of $\\sigma$ or large value of $\\gamma$, area of influence is close to any support vectors. Clearly support vector influence increases as $\\sigma$ increase or $\\gamma$ decreases. It is similary to effect of $\\sigma$ in gussian distribution.\n",
    "\n",
    "\n",
    "\n",
    "Search for $\\sigma.$ \n",
    "\n",
    "<font color = 'red'>Notice choose right value of $\\sigma$ is very important in \n",
    "RBF kernel. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUPPORT_VECTOR_WEIGHT_THR = 1e-4\n",
    "\n",
    "best_val_accuracy = -np.inf\n",
    "best_support_vectors_info= None\n",
    "for sigma in np.linspace(.05, 15, 15):    \n",
    "    #Let's iterate over sample and fill matrix K\n",
    "    # Maybe we can do following operations in matrix form.\n",
    "    # Going with two loops right now\n",
    "    for i, x1 in enumerate(X_train):\n",
    "        for j, x2 in enumerate(X_train):\n",
    "            #print(x1.shape, x2.shape)\n",
    "            K[i,j] = rbf_kernel(x1, x2, sigma)\n",
    "    # Write code here\n",
    "    P = cvxopt.matrix(np.outer(y_train, y_train)*K )           \n",
    "    # Let'solve the optimization problem using qudratic solver from cvxopt\n",
    "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.array(sol['x'])\n",
    "    svec_loc = alphas > SUPPORT_VECTOR_WEIGHT_THR\n",
    "    svec_alphas = alphas[svec_loc]\n",
    "    \n",
    "    # svecs contains our support vector\n",
    "    svecs, svec_labels = X_train[ np.ravel(svec_loc),:], y_train[ np.ravel(svec_loc)]\n",
    "    # evaluating the intercept for all the support vectors.\n",
    "    svec_wo_pos = []\n",
    "    svec_wo_neg = []\n",
    "    for svec1 ,lbl1 in zip(svecs, svec_labels):\n",
    "        val = 0\n",
    "        for alpha, svec2, lbl2 in  zip(svec_alphas, svecs, svec_labels):\n",
    "            val +=  alpha*lbl2*rbf_kernel(svec1,svec2, sigma )\n",
    "        if lbl1== +1:    \n",
    "            svec_wo_pos.append( val)\n",
    "        elif lbl1 == -1:\n",
    "            svec_wo_neg.append(val)\n",
    "        else:\n",
    "            print('erro support vector label in not +ve or -ve')\n",
    "\n",
    "    #Let use first/ or mean?\n",
    "    #wo = np.mean(svec_wo)\n",
    "    wo = -(max(svec_wo_neg) + min(svec_wo_pos))/2.0\n",
    "    \n",
    "    val_acc = np.mean([ y == predict_label(x, svec_alphas, svecs, svec_labels, wo, sigma)\n",
    "                       for  x, y in  zip(X_val, y_val)])\n",
    "    print('sigma = {} validation accuracy = {}'.format(sigma,val_acc))\n",
    "    if val_acc > best_val_accuracy:\n",
    "        print('updating best model current val {} previous val {}'.format(val_acc, best_val_accuracy))\n",
    "        best_support_vector_info = (sigma, svec_alphas,svecs, svec_labels, wo)\n",
    "        best_val_accuracy = val_acc\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# picking best sigma for accuracy on validation set \n",
    "\n",
    "best_sigma, svec_alphas, svecs, svec_labels, wo = best_support_vector_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3(2 point) write code  to evalue validation and test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4(1 point). Use sklearn kernel SVM with soft margin interface. Select best model and evaluate accuracy on test set using SVM from  the library\n",
    "\n",
    "Check your accuracy value matches\n",
    "\n",
    "Hint\n",
    "\n",
    "- You need to use large value of C to simulate hard margin SVM\n",
    "- gamma= 1/(2*best_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = ## write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
