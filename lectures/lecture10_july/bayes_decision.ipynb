{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Decision Theory\n",
    "\n",
    "- Helps us in identifying tradeoff between various pattern  classification using probability and associated cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scenario. \n",
    "- Let say you work are hired in a quality control depertment of a company.\n",
    "- Your job(action) is   accept ($a_1$) or reject($a_2$) a product based on some measureable features $\\mathbf{x} = [\\text{length, width, weight, color}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before starting on the job\n",
    "\n",
    "- You start with historical record of accepted and rejected product.\n",
    "- You estimate a priori or prior probability of a prduct being accepted $P(a_1)$ or $P(a_2)$ rejected, which can be fraction of accpeted and rejected product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decision rule if we only have $P(a_1), P(a_2)$ from historical data\n",
    "\n",
    "<font color=\"red\" > prior information and cost of mis-classification is same </font>\n",
    "\n",
    "i.e decide $a_1$ if $P(a_1) \\gt P(a_2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you comment on this rule ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will end up classifying new product either accept or reject(it look ok under given conditions)\n",
    "- What if prior are equal $P(a_1) == P(a_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let say we observe a new product with feature $\\mathbf{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given historical data can we do better?\n",
    "\n",
    "- Can we estimate $P(a_1|\\mathbf{x})$ and $P(a_2|\\mathbf{x})$ and have better decision rule?\n",
    "\n",
    " posterior probability $P(a_i|\\mathbf{x})$ given observable feature $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center > **Yes we know bayes rule** </center>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "$P(a_1|\\mathbf{x}) = \\frac{P(a_1, \\mathbf{x})}{P(\\mathbf{x})} =\\frac{P(a_1, \\mathbf{x})}{\\sum_{a= a_1, a_2}P(a,\\mathbf{x})}=  \\frac{P(\\mathbf{x}|a_1) P(a_1)}{P(\\mathbf{x})} =\\frac{P(\\mathbf{x}|a_1)P(a_1)}{\\sum_{i= 1}^{2}P(\\mathbf{x}|a_i) P(a_i)} $\n",
    "    </center>\n",
    "    \n",
    "<center> likelihood $P(\\mathbf{x}|a_i)$ and prior $P(a_i)$ control posterior $P(a_i|\\mathbf{x})$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "**And how to make the decision**\n",
    "    </center>\n",
    "    <br>\n",
    "<center>\n",
    "use posterior to decide correct class (state of nature)\n",
    "    $ a = \\arg \\max_{a_i} P(a_i|\\mathbf{x})$\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Probability of error\n",
    "\n",
    "$P(error|\\mathbf{x}) = \\begin{cases}\n",
    "      P(a_1|\\mathbf{x}), & \\text{if we choose}\\; a_2 \\\\\n",
    "      P(a_2|\\mathbf{x}), & \\text{if we choose}\\; a_1\n",
    "    \\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayes decision rule combines prior and likelihood(under same loss)\n",
    "We can minimize the probability of error by using a decision based on  posterior\n",
    "\n",
    "- choose $a_1$ if $P(a_1|\\mathbf{x}) \\gt P(a_2|\\mathbf{x})$ which is same as\n",
    "- choose $a_1$ if $P(\\mathbf{x}|a_1) P(a_1) \\gt P(\\mathbf{x}|a_2) P(a_2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> what happen if uniform prior or same likelihood </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "Infact $P(error) = \\int_{-\\infty}^{+\\infty} P(error|\\mathbf{x} P(\\mathbf{x})) d\\mathbf{x}$ average probability of error is minimized too\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's talk in a more general setting and more generic cost or loss function\n",
    "- We have $k$ possible classes $\\{y_1, y_2, \\cdots , y_k \\}$, also called states of nature.\n",
    "- And action we can take $\\{a_1, a_2, \\cdots, a_N \\}$\n",
    "- $L(y_i, a_j)$ is loss incured taking action $a_j$ when state of nature is $y_i$. A measure of compatibility between nature state $c_i$ and action taken $a_j$\n",
    "    + like miss classification loss/0-1 loss  $L(y_i, a_j) = \\mathbb{I}(y_i \\ne a_j)$ or squared $(y_i -a_j)^2$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " Let's say having observed $\\mathbf{x}$ we take action $a_j$\n",
    " Then the **risk function or posterior expected loss or conditional risk** is\n",
    " \n",
    " $R(a_j|\\mathbf{x}) =  \\mathbb{E}_{p(y|\\mathbf{x})}[L(y, a)] =\\sum_{i=1}^{k} L(y_i, a_j) P(y_i|\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What action we should take after observing $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>Clearly which minimized the risk or mathematically\n",
    "<br> <br>\n",
    "optimal action/decision procedure/ policy  is = $arg \\min _{a_j} R(a_j|\\mathbf{x})$\n",
    "<br> <br>\n",
    "Hence the **Bayes estimator, also called the Bayes decision rule or minimum risk classifier**, is given\n",
    "by <br> <br>\n",
    " <font size = 6>\n",
    "$arg \\min_a \\mathbb{E}_{p(y|\\mathbf{x})}L(y ,a)$ </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's build some Bayes estimators for common loss functions in machine learning\n",
    "\n",
    "## MAP estimate minimizes 0-1 loss(most commly used in classification problem)\n",
    "\n",
    "$L(y_i, a_j) = \\mathbb{I}(y_i \\ne a_j) = \\begin{cases}\n",
    "      0, & \\text{if }\\; y_i = a_j \\\\\n",
    "      1, & \\text{if}\\; y_i \\ne a_j\n",
    "    \\end{cases} \n",
    "    $\n",
    "    \n",
    "    \n",
    "$y_i \\text{ is true label and } a_i = \\hat{y}_j $ is an estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$R(a_j|\\mathbf{x}) = \\sum_{i =1}^{i = k}L(y_i, a_j) P(y_i|\\mathbf{x})= \\sum_{i \\ne j} P(y_i|\\mathbf{x})  = 1- P(y_j|\\mathbf{x})$\n",
    "\n",
    "\n",
    "\n",
    "As per bayes decision rule choose choose action which minimized the risk\n",
    "\n",
    "or miximized posterior(mode of posterior) or MAP estimate or $arg \\max_{y} P(y|\\mathbf{x})$\n",
    "\n",
    "See how nicely bayes decision rule justifies our action of choosing maximum posterior probability class under equal loss of miss classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The false positive vs false negative tradeoff\n",
    "Let's focus on binary classification like accept/reject, success/failure etc. \n",
    "- Two type of error we can make\n",
    "    + FP(false positive) i.e $a = \\hat{y} = 1$ when truth is $y =0$\n",
    "    + FN (false negative) i.e $a = \\hat{y} = 0$ when truth is $y =1$\n",
    "0-1 loss consider these two kind of error same but let use more generic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "|       | $\\hat{y} =1$           | $\\hat{y} =0$  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| y=1      | 0 |$L_{FN}$ |\n",
    "| y=0      | $L_{FP}$      |   0 |\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Expected loss/risk for two action is \n",
    "- $R(\\hat{y} =0|\\mathbf{x})$ = $L_{FN}P(y=1|\\mathbf{x}$\n",
    "\n",
    "- $R(\\hat{y} =1|\\mathbf{x})$ = $L_{FP}P(y=0|\\mathbf{x})$\n",
    "\n",
    "Choose class $\\hat{y} =1$ if\n",
    "\n",
    "$R(\\hat{y} =0|\\mathbf{x})$ > $R(\\hat{y} =1|\\mathbf{x})$\n",
    "\n",
    "$\\frac{P(y=1|\\mathbf{x}}{P(y=0|\\mathbf{x}} \\gt \\frac{L_{FP}}{L_{FN}} = \\tau$\n",
    "\n",
    "If $L_{FP} = c L_{FN}$ then we will pick $\\hat{y} = 1$ iff $P(y=1|\\mathbf{x} > \\tau$ where $\\tau = \\frac{c}{(1+c)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ROC(receiver operating characteristic) curve and AUC(Area under the curve)\n",
    "\n",
    "<center><font size =6 color = \"green\">ROC curve = plot of (TPR and FPR) </font> </center>\n",
    "\n",
    "Previously choice $\\hat{y}$\n",
    "depends on $\\tau$. We can do better and analyize FP and FN for without comiting to a particular threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Let's say we have training data $\\mathcal{D} = {\\mathbf{x_i}, y_i}$ and build our machine learning model $P(y|\\mathbf{x})$ for binary classification $y_i \\in \\{0, 1\\}$.\n",
    "- As we talked previously and shown in the coding HW, we can massage $P(y|\\mathbf{x})$ using monotonic functions to get a economical disriminant function $f(\\mathbf{x})$\n",
    "- $f(\\mathbf{x})$ measure our confidence score for $\\hat{y} =1$\n",
    "- Our decision rule is, $y_i = \\mathbb{I}(f(\\mathbf{x}) > \\tau)$ for some threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each value of $\\tau$ we get following table(**Confusion matrix**) measuring our error in term of numer of true positive(TP), false positive(FP), true negative(TN) and false negative(FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Confusion Matrix for a particular value of $\\tau$\n",
    "|       | $y=1$           | $y=0$  |     |\n",
    "| ------------- |:-------------:| -----:|:---|\n",
    "| $\\hat{y}$=1      | TP |FP | $\\hat{N}_{+}$ = TP + FN   |    |\n",
    "| $\\hat{y}$=0      | FN      |   TN |     $\\hat{N}_{-}$ = TN + FN|\n",
    "|------------- |:-------------:| -----:|:-----|\n",
    "|    | $N_{+}$ = TP + FN    |  $N_{-}$ = TN + FP   | N= TP+FN +TN+FP    |\n",
    "\n",
    "\n",
    "True positive rate(**TPR**) = $\\frac{TP}{N_+} \\approx p(\\hat{y} =1| y =1)$\n",
    "\n",
    "false positive rate (**FPR**)(type I error rate) = $\\frac{FP}{N_{-}} \\approx p(\\hat{y} =1| y =0)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Estimate of $P(\\hat{y}|y)$ from confusion matrix\n",
    "|       | $y=1$           | $y=0$  |  \n",
    "| ------------- |:-------------:| -----:|\n",
    "| $\\hat{y}$=1      | TPR=$\\frac{TP}{N_+}$=sensitive=recall |FPR=$\\frac{FP}{N_{-}}$=type-1 error   | \n",
    "| $\\hat{y}$=0      | FNR=$\\frac{FN}{N_+}$=type-II error      | TNR=$\\frac{TN}{N_{-}}$=specificity   |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <font size=5 color = \"Maroon\">Calculate TPR and FPR for set of threshold $\\tau_i$ nd plot them = ROC</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"fig5_15a.jpg\" alt=\"Smiley face\" >\n",
    "\n",
    "credit: image  from Kevin Muphy book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also used Area Under the Receiver Operating Characteristic curve to compare two classifer(like A, B in previous picture)\n",
    "- AUC = Area Under the Curve.\n",
    "- AUROC = Area Under the Receiver Operating Characteristic curve\n",
    "\n",
    "AUC is used most of the time to mean AUROC, which is kind of a bad practice but widely used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting  roc in python see\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
